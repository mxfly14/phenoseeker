{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "\n",
    "def compute_enrichment_factor_at_n(hit_embedding, all_embeddings, all_labels, n_percent=1):\n",
    "    \"\"\"\n",
    "    Calculate the enrichment factor (EF) at n% for a given 'hit' embedding, excluding the control positive.\n",
    "\n",
    "    Parameters:\n",
    "    - hit_embedding: numpy array, the embedding of the 'hit' sample.\n",
    "    - all_embeddings: numpy array, embeddings of all samples.\n",
    "    - all_labels: numpy array, binary labels for all samples (1 for 'hit', 0 otherwise).\n",
    "    - n_percent: float, the percentage (0-100) of the dataset to consider for EF calculation.\n",
    "\n",
    "    Returns:\n",
    "    - float, the Enrichment Factor at n%.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity of the hit with all samples\n",
    "    similarities = cosine_similarity(hit_embedding.reshape(1, -1), all_embeddings).flatten()\n",
    "\n",
    "    # Rank indices based on similarity\n",
    "    ranked_indices = np.argsort(-similarities)  # Descending order\n",
    "    ranked_labels = all_labels[ranked_indices]  # Get labels of ranked samples\n",
    "\n",
    "    # Remove the first entry (control positive)\n",
    "    ranked_indices = ranked_indices[1:]  # Exclude the first index\n",
    "    ranked_labels = ranked_labels[1:]    # Exclude the first label\n",
    "\n",
    "    # Calculate top n% cutoff\n",
    "    n_top = max(1, int(len(ranked_labels) * (n_percent / 100)))  # At least 1 sample\n",
    "\n",
    "    # Count hits in the top n% of ranked samples\n",
    "    hits_in_top_n = np.sum(ranked_labels[:n_top])\n",
    "\n",
    "    # Total hits in the dataset\n",
    "    total_hits = np.sum(all_labels)\n",
    "\n",
    "    # Compute EF\n",
    "    if total_hits == 0:  # Avoid division by zero\n",
    "        return 0.0\n",
    "    enrichment_factor = (hits_in_top_n / n_top) / (total_hits / len(all_labels))\n",
    "\n",
    "    return enrichment_factor\n",
    "def evaluate_EF_df(df_test, n):\n",
    "    all_labels = (df_test['Active'] == True).astype(int).values\n",
    "    hits = df_test[df_test['Active']]\n",
    "\n",
    "    hits_embeddings = np.stack(hits['Embeddings_mean'].values).astype(np.float16)\n",
    "    all_embeddings = np.stack(df_test['Embeddings_mean'].values).astype(np.float16)\n",
    "    enrichment_factor = Parallel(n_jobs=40)(\n",
    "        delayed(compute_enrichment_factor_at_n)(hit_embedding, all_embeddings, all_labels, n)\n",
    "        for hit_embedding in tqdm(hits_embeddings)\n",
    "    )\n",
    "\n",
    "    mEF = np.mean(enrichment_factor)\n",
    "    maxEF = np.max(enrichment_factor)\n",
    "    return mEF, maxEF\n",
    "\n",
    "def compute_phenotypic_similarity(df):\n",
    "    embeddings = np.stack(df['Embeddings_mean'])  \n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "def hierarchical_clustering_and_visualization(df, similarity_matrix, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Effectue un clustering hiérarchique sur les molécules et visualise les résultats.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant les molécules et leurs métadonnées.\n",
    "        similarity_matrix (np.ndarray): Matrice de similarité entre les molécules.\n",
    "        threshold (float): Seuil pour déterminer les clusters à partir du dendrogramme.\n",
    "    \"\"\"\n",
    "    # Convertir la matrice de similarité en matrice de dissimilarité\n",
    "    dissimilarity = 1 - similarity_matrix\n",
    "\n",
    "    # Créer un linkage pour le clustering hiérarchique\n",
    "    linkage_matrix = linkage(dissimilarity, method='average')\n",
    "\n",
    "    # Afficher le dendrogramme\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(linkage_matrix, labels=df['Metadata_JCP2022'].values, leaf_rotation=90)\n",
    "    plt.title(\"Dendrogramme de clustering hiérarchique\")\n",
    "    plt.xlabel(\"Molécules\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Seuil {threshold}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Déterminer les clusters à partir du seuil\n",
    "    clusters = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "    df['Cluster'] = clusters\n",
    "    df_sorted = df.sort_values(by='Cluster').reset_index(drop=True)\n",
    "\n",
    "    # Visualisation des molécules par cluster\n",
    "    for cluster_id in sorted(df['Cluster'].unique()):\n",
    "        cluster_df = df[df['Cluster'] == cluster_id]\n",
    "        mols = [Chem.MolFromInchi(inchi) for inchi in cluster_df['Metadata_InChI']]\n",
    "        legends = list(cluster_df['Metadata_JCP2022'].astype(str))\n",
    "\n",
    "        print(f\"Cluster {cluster_id} - {len(cluster_df)} molécules\")\n",
    "        img = Draw.MolsToGridImage(mols, molsPerRow=5, subImgSize=(200, 200), legends=legends)\n",
    "        display(img)\n",
    "    return df_sorted\n",
    "\n",
    "def visualize_similarity_matrix(df, similarity_matrix, fontsize_sim=14):\n",
    "    \"\"\"\n",
    "    Visualise la matrice de similarité cosinus avec les valeurs affichées.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame contenant la colonne Metadata_JCP2022.\n",
    "        similarity_matrix: numpy array, matrice de similarité cosinus.\n",
    "    \"\"\"\n",
    "    ids = df['Metadata_JCP2022'].values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Matrice de similarité\n",
    "    cax = ax.imshow(similarity_matrix, cmap='viridis', interpolation='none')  # Suppression de l'interpolation\n",
    "    ax.set_title('Cosine Similarity Matrix', fontsize=14)\n",
    "    ax.set_xticks(range(len(ids)))\n",
    "    ax.set_yticks(range(len(ids)))\n",
    "    ax.set_xticklabels(ids, rotation=90, fontsize=8)\n",
    "    ax.set_yticklabels(ids, fontsize=8)\n",
    "\n",
    "    # Supprimer les lignes blanches entre les pixels\n",
    "    ax.set_xticks([], minor=True)\n",
    "    ax.set_yticks([], minor=True)\n",
    "    ax.grid(False)  # Désactiver les grilles\n",
    "\n",
    "    # Ajouter les valeurs dans les cases\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        for j in range(similarity_matrix.shape[1]):\n",
    "            value = f\"{similarity_matrix[i, j]:.2f}\"  # Formater à 2 décimales\n",
    "            ax.text(j, i, value, ha='center', va='center', fontsize=fontsize_sim, color='white')\n",
    "\n",
    "    # Ajouter une barre de couleur\n",
    "    plt.colorbar(cax, ax=ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_phenom = pd.read_parquet('/home/maxime/data/jump_embeddings/metadata_dinov2_g.parquet')\n",
    "df_phenom = pd.read_parquet('/projects/synsight/data/jump_embeddings/wells_embeddings/openphenom/metadata_openphenom.parquet')\n",
    "\n",
    "df_jump = df_phenom[[\"Metadata_JCP2022\", \"Metadata_InChI\"]].drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_phenom = pd.read_parquet('/projects/synsight/data/openphenom/norm_2_compounds_embeddings.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = AllChem.GetMorganGenerator(radius=2, fpSize=2048, includeChirality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inchi_to_fp(inchi):\n",
    "    \"\"\"Convert InChI string to RDKit Morgan fingerprint.\"\"\"\n",
    "    mol = Chem.MolFromInchi(inchi)\n",
    "    if mol:\n",
    "        return mg.GetFingerprint(mol)\n",
    "    return None\n",
    " \n",
    "def smiles_to_fp(smiles):\n",
    "    \"\"\"Convert SMILES to RDKit fingerprint.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return mg.GetFingerprint(mol)\n",
    "    return None\n",
    "\n",
    "def bulk_tanimoto_similarity(query_fp, list_of_fps):\n",
    "    \"\"\"Compute Tanimoto similarity efficiently in bulk.\"\"\"\n",
    "    list_of_fps = list(list_of_fps)  # Ensure it's a Python list\n",
    "    return DataStructs.BulkTanimotoSimilarity(query_fp, list_of_fps)\n",
    "\n",
    "def compute_similarity(query_fp, list_of_fps_jump):\n",
    "    \"\"\"Compute Tanimoto similarity between a query InChI and a list of InChIs.\"\"\"\n",
    "\n",
    "    if query_fp is None:\n",
    "        raise ValueError(\"Invalid query\")\n",
    "    query_fp = smiles_to_fp(query_fp)\n",
    "    list_of_fps = [fp for fp in list_of_fps_jump if fp is not None]  # Filter out None values\n",
    "    \n",
    "    return bulk_tanimoto_similarity(query_fp, list_of_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_fps_jump = [inchi_to_fp(inchi) for inchi in tqdm(df_jump['Metadata_InChI'].to_list())]\n",
    "df_jump['Fps'] = list_of_fps_jump\n",
    "df_jump.dropna(subset='Fps', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import mols from lit-pcba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_smi_files(base_path):\n",
    "    \"\"\"\n",
    "    Charge les fichiers actives.smi et inactives.smi d'un dossier et retourne un dictionnaire de DataFrames.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Le chemin vers le dossier contenant les sous-dossiers avec les fichiers .smi.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire où chaque clé est le nom du sous-dossier et la valeur est un DataFrame.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "\n",
    "    # Liste tous les sous-dossiers\n",
    "    for folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "        # Vérifie si c'est bien un dossier\n",
    "        if os.path.isdir(folder_path):\n",
    "            actives_file = os.path.join(folder_path, \"actives_final.ism\")\n",
    "            inactives_file = os.path.join(folder_path, \"decoys_final.ism\")\n",
    "\n",
    "            all_data = []\n",
    "\n",
    "            # Lire actives.smi\n",
    "            if os.path.exists(actives_file):\n",
    "                df_actives = pd.read_csv(actives_file, sep=\" \", names=[\"smiles\", \"id_dude\", 'ChEMBL_id'])\n",
    "                df_actives[\"Active\"] = True\n",
    "                all_data.append(df_actives[[\"smiles\", \"id_dude\", \"Active\"]])\n",
    "\n",
    "            # Lire inactives.smi\n",
    "            if os.path.exists(inactives_file):\n",
    "                df_inactives = pd.read_csv(inactives_file, sep=\" \", names=[\"smiles\", \"id_dude\"])\n",
    "                df_inactives[\"Active\"] = False\n",
    "                all_data.append(df_inactives)\n",
    "\n",
    "            # Si on a des données, on les stocke\n",
    "            if all_data:\n",
    "                data_dict[folder] = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# Chemin vers ton dossier \"data\"\n",
    "base_path = \"../data/DUDE\"\n",
    "\n",
    "# Charger les données\n",
    "data_dict = load_smi_files(base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_smiles = []\n",
    "for key, df in tqdm(data_dict.items()):\n",
    "    unique_smiles = unique_smiles + df[\"smiles\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['aa2ar']['smiles'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "def load_parquets_files(base_path):\n",
    "    \"\"\"\n",
    "    Charge les fichiers actives.smi et inactives.smi d'un dossier et retourne un dictionnaire de DataFrames.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Le chemin vers le dossier contenant les sous-dossiers avec les fichiers .smi.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire où chaque clé est le nom du sous-dossier et la valeur est un DataFrame.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for file in os.listdir(base_path):\n",
    "        if Path(file).suffix == '.parquet':\n",
    "            name = Path(file).stem.split('_')[1:][0]\n",
    "            df = pd.read_parquet(Path(base_path) / Path(file))\n",
    "            data_dict[name] = df\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# Chemin vers ton dossier \"data\"\n",
    "base_path = \"../scripts\"\n",
    "\n",
    "# Charger les données\n",
    "data_dict_jump = load_parquets_files(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Determine the number of subplots needed\n",
    "n_keys = len(data_dict)\n",
    "n_cols = 6  # adjust number of columns as needed\n",
    "n_rows = math.ceil(n_keys / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 4))\n",
    "# If there is only one row or one plot, ensure axes is iterable\n",
    "if n_keys == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Loop through each key and dataframe and plot on the corresponding subplot axis\n",
    "for ax, (key, df) in zip(axes, data_dict.items()):\n",
    "    active_counts = df['Active'].value_counts().reset_index()\n",
    "    active_counts.columns = ['Active', 'Count']\n",
    "\n",
    "    sns.barplot(x='Active', y='Count', data=active_counts, palette='viridis', ax=ax)\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel(\"Active Status\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    \n",
    "    # Add text labels on top of each bar\n",
    "    for index, row in active_counts.iterrows():\n",
    "        ax.text(index, row['Count'], row['Count'], color='black', ha=\"center\", va='bottom')\n",
    "\n",
    "# Remove any empty subplots if number of keys is less than n_rows*n_cols\n",
    "for ax in axes[len(data_dict):]:\n",
    "    ax.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 1\n",
    "filtered_data = {}\n",
    "# Afficher un exemple\n",
    "for key, df in data_dict_jump.items():\n",
    "    filtered_df = df[df['tanimoto_similarity'] >= similarity_threshold].drop_duplicates(subset='smiles')\n",
    "    filtered_df = filtered_df.merge(df_phenom, left_on='closest_jcp', right_on='Metadata_JCP2022')[['id_lit_pcba', 'Active', 'Metadata_JCP2022', 'Embeddings_mean', 'smiles', 'Metadata_InChI']]\n",
    "    filtered_data[key] = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Determine the number of subplots needed\n",
    "n_keys = len(filtered_data)\n",
    "n_cols = 4  # adjust number of columns as needed\n",
    "n_rows = math.ceil(n_keys / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 4))\n",
    "# If there is only one row or one plot, ensure axes is iterable\n",
    "if n_keys == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Loop through each key and dataframe and plot on the corresponding subplot axis\n",
    "for ax, (key, df) in zip(axes, filtered_data.items()):\n",
    "    active_counts = df['Active'].value_counts().reset_index()\n",
    "    active_counts.columns = ['Active', 'Count']\n",
    "\n",
    "    sns.barplot(x='Active', y='Count', data=active_counts, palette='viridis', ax=ax)\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel(\"Active Status\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    \n",
    "    # Add text labels on top of each bar\n",
    "    for index, row in active_counts.iterrows():\n",
    "        ax.text(index, row['Count'], row['Count'], color='black', ha=\"center\", va='bottom')\n",
    "\n",
    "# Remove any empty subplots if number of keys is less than n_rows*n_cols\n",
    "for ax in axes[len(filtered_data):]:\n",
    "    ax.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Determine the number of subplots needed\n",
    "n_keys = len(filtered_active_data)\n",
    "n_cols = 3  # adjust number of columns as needed\n",
    "n_rows = math.ceil(n_keys / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, n_rows * 4))\n",
    "# If there is only one row or one plot, ensure axes is iterable\n",
    "if n_keys == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Loop through each key and dataframe and plot on the corresponding subplot axis\n",
    "for ax, (key, df) in zip(axes, filtered_active_data.items()):\n",
    "    active_counts = df['Active'].value_counts().reset_index()\n",
    "    active_counts.columns = ['Active', 'Count']\n",
    "\n",
    "    sns.barplot(x='Active', y='Count', data=active_counts, palette='viridis', ax=ax)\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel(\"Active Status\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    \n",
    "    # Add text labels on top of each bar\n",
    "    for index, row in active_counts.iterrows():\n",
    "        ax.text(index, row['Count'], row['Count'], color='black', ha=\"center\", va='bottom')\n",
    "\n",
    "# Remove any empty subplots if number of keys is less than n_rows*n_cols\n",
    "for ax in axes[len(filtered_data):]:\n",
    "    ax.remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "# Filter the dictionary:\n",
    "# This comprehension creates a new dictionary (filtered_active_data) containing only the keys\n",
    "# for which the sum of the 'Active' column (i.e. count of active rows) is less than n.\n",
    "filtered_active_data = {\n",
    "    key: df for key, df in filtered_data.items() if df['Active'].sum() > n\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming evaluate_EF_df is defined as, for example:\n",
    "# def evaluate_EF_df(df, percent):\n",
    "#     # Compute the mean EF (mEF) and the maximum EF (maxEF) for the given percentage of selected molecules\n",
    "#     return mEF, maxEF\n",
    "\n",
    "# Calculate the enrichment factors for each DataFrame in filtered_active_data\n",
    "results = []\n",
    "for key, df in filtered_active_data.items():\n",
    "    mean_EF, max_EF = evaluate_EF_df(df, 5)  # Here, 1 represents 1% of selected molecules\n",
    "    results.append({'Key': key, 'Mean EF': mean_EF, 'Max EF': max_EF})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Transform the DataFrame to a long format suitable for seaborn's barplot\n",
    "results_long = results_df.melt(id_vars='Key', value_vars=['Mean EF', 'Max EF'], \n",
    "                               var_name='EF Type', value_name='EF Value')\n",
    "\n",
    "# Set a style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_plot = sns.barplot(data=results_long, x='Key', y='EF Value', hue='EF Type', palette='viridis')\n",
    "\n",
    "# Add a red horizontal dashed line at EF = 1 (the random baseline)\n",
    "plt.axhline(y=1, color='red', linestyle='--', label='Random EF = 1')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Enrichment Factors (5% of Selected Molecules)\")\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Enrichment Factor\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Retrieve current legend handles and labels\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# If the red line's label is not already included, add it manually\n",
    "if 'Random EF = 1' not in labels:\n",
    "    from matplotlib.lines import Line2D\n",
    "    handles.append(Line2D([0], [0], color='red', linestyle='--'))\n",
    "    labels.append('Random EF = 1')\n",
    "\n",
    "plt.legend(handles=handles, labels=labels, title='EF Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in filtered_active_data.items():\n",
    "    print(key)\n",
    "    mEF, maxEF = evaluate_EF_df(df, 5)\n",
    "    print(f\"Mean Enrichment Factor (mEF): {mEF}\")\n",
    "    print(f\"Max Enrichment Factor (mEF): {maxEF}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in filtered_active_data.items():\n",
    "    print(key)\n",
    "    mEF, maxEF = evaluate_EF_df(df, 3)\n",
    "    print(f\"Mean Enrichment Factor (mEF): {mEF}\")\n",
    "    print(f\"Max Enrichment Factor (mEF): {maxEF}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_active_data['VDR']\n",
    "df = df[df['Active']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = compute_phenotypic_similarity(df)\n",
    "df = hierarchical_clustering_and_visualization(df, similarity_matrix, threshold=1.7)\n",
    "similarity_matrix = compute_phenotypic_similarity(df)\n",
    "visualize_similarity_matrix(df, similarity_matrix, fontsize_sim=6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
