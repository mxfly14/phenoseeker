{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_phenom = pd.read_parquet('/home/maxime/data/jump_embeddings/metadata_dinov2_g.parquet')\n",
    "df_phenom = pd.read_parquet('/projects/synsight/data/jump_embeddings/wells_embeddings/openphenom/metadata_openphenom.parquet')\n",
    "\n",
    "df_jump = df_phenom[[\"Metadata_JCP2022\", \"Metadata_InChI\"]].drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = AllChem.GetMorganGenerator(radius=2, fpSize=2048, includeChirality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inchi_to_fp(inchi):\n",
    "    \"\"\"Convert InChI string to RDKit Morgan fingerprint.\"\"\"\n",
    "    mol = Chem.MolFromInchi(inchi)\n",
    "    if mol:\n",
    "        return mg.GetFingerprint(mol)\n",
    "    return None\n",
    " \n",
    "def smiles_to_fp(smiles):\n",
    "    \"\"\"Convert SMILES to RDKit fingerprint.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return mg.GetFingerprint(mol)\n",
    "    return None\n",
    "\n",
    "def bulk_tanimoto_similarity(query_fp, list_of_fps):\n",
    "    \"\"\"Compute Tanimoto similarity efficiently in bulk.\"\"\"\n",
    "    list_of_fps = list(list_of_fps)  # Ensure it's a Python list\n",
    "    return DataStructs.BulkTanimotoSimilarity(query_fp, list_of_fps)\n",
    "\n",
    "def compute_similarity(query_fp, list_of_fps_jump, query_type='smiles'):\n",
    "    \"\"\"Compute Tanimoto similarity between a query InChI and a list of InChIs.\"\"\"\n",
    "\n",
    "    if query_fp is None:\n",
    "        raise ValueError(\"Invalid query\")\n",
    "    \n",
    "    list_of_fps = [fp for fp in list_of_fps_jump if fp is not None]  # Filter out None values\n",
    "    \n",
    "    return bulk_tanimoto_similarity(query_fp, list_of_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_fps_jump = [inchi_to_fp(inchi) for inchi in tqdm(df_jump['Metadata_InChI'].to_list())]\n",
    "df_jump['Fps'] = list_of_fps_jump\n",
    "df_jump.dropna(subset='Fps', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import mols from lit-pcba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_smi_files(base_path):\n",
    "    \"\"\"\n",
    "    Charge les fichiers actives.smi et inactives.smi d'un dossier et retourne un dictionnaire de DataFrames.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Le chemin vers le dossier contenant les sous-dossiers avec les fichiers .smi.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire o√π chaque cl√© est le nom du sous-dossier et la valeur est un DataFrame.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "\n",
    "    # Liste tous les sous-dossiers\n",
    "    for folder in os.listdir(base_path):\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "        # V√©rifie si c'est bien un dossier\n",
    "        if os.path.isdir(folder_path):\n",
    "            actives_file = os.path.join(folder_path, \"actives.smi\")\n",
    "            inactives_file = os.path.join(folder_path, \"inactives.smi\")\n",
    "\n",
    "            all_data = []\n",
    "\n",
    "            # Lire actives.smi\n",
    "            if os.path.exists(actives_file):\n",
    "                df_actives = pd.read_csv(actives_file, sep=\" \", names=[\"smiles\", \"id_lit_pcba\"])\n",
    "                df_actives[\"Active\"] = True\n",
    "                all_data.append(df_actives)\n",
    "\n",
    "            # Lire inactives.smi\n",
    "            if os.path.exists(inactives_file):\n",
    "                df_inactives = pd.read_csv(inactives_file, sep=\" \", names=[\"smiles\", \"id_lit_pcba\"])\n",
    "                df_inactives[\"Active\"] = False\n",
    "                all_data.append(df_inactives)\n",
    "\n",
    "            # Si on a des donn√©es, on les stocke\n",
    "            if all_data:\n",
    "                data_dict[folder] = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# Chemin vers ton dossier \"data\"\n",
    "base_path = \"../data\"\n",
    "\n",
    "# Charger les donn√©es\n",
    "data_dict = load_smi_files(base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Afficher un exemple\n",
    "for key, df in data_dict.items():\n",
    "    if len(df)<6000:\n",
    "        print(f\"\\nüìÇ {key} (Total: {len(df)} mol√©cules)\")\n",
    "        list_of_fps_jump = [smiles_to_fp(smi) for smi in tqdm(df['smiles'].to_list())]\n",
    "        df['Fps'] = list_of_fps_jump\n",
    "        df.dropna(subset='Fps', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def compute_best_match(query_fp, list_of_fps_jump, jump_ids):\n",
    "    \"\"\"\n",
    "    Trouve la mol√©cule de df_jump la plus similaire √† une mol√©cule donn√©e.\n",
    "\n",
    "    Args:\n",
    "        query_fp (str): L'empreinte de la mol√©cule √† comparer.\n",
    "        list_of_fps_jump (list): Liste des empreintes des mol√©cules JUMP.\n",
    "        jump_ids (list): Liste des ID des mol√©cules JUMP.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (meilleure similarit√©, ID de la mol√©cule correspondante)\n",
    "    \"\"\"\n",
    "    similarities = compute_similarity(query_fp, list_of_fps_jump)  # Calcule la similarit√©\n",
    "    best_index = max(range(len(similarities)), key=lambda i: similarities[i])  # Trouve l'index du max\n",
    "    return similarities[best_index], jump_ids[best_index]  # Retourne (similarit√©, ID)\n",
    "\n",
    "def find_best_match_parallel(df, df_jump, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Utilise joblib pour parall√©liser la recherche des meilleures correspondances.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contenant les mol√©cules √† comparer.\n",
    "        df_jump (pd.DataFrame): DataFrame contenant les mol√©cules de r√©f√©rence.\n",
    "        n_jobs (int): Nombre de c≈ìurs √† utiliser (-1 = tous les c≈ìurs).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Le DataFrame original avec deux nouvelles colonnes :\n",
    "                      - Best_Similarity: la similarit√© max trouv√©e\n",
    "                      - Best_ID_Jump: l'ID de la mol√©cule correspondante dans df_jump\n",
    "    \"\"\"\n",
    "    list_of_fps_jump = df_jump['Fps'].to_list()  # Liste des empreintes JUMP\n",
    "    jump_ids = df_jump['Metadata_JCP2022'].to_list()  # Liste des ID JUMP\n",
    "\n",
    "    # Utilisation de joblib pour parall√©liser le calcul des similarit√©s\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "        delayed(compute_best_match)(row['Fps'], list_of_fps_jump, jump_ids) for _, row in tqdm(df.iterrows(), total=df.shape[0])\n",
    "    )\n",
    "\n",
    "    # Extraction des r√©sultats\n",
    "    best_similarities, best_ids = zip(*results)\n",
    "\n",
    "    df['Best_Similarity'] = best_similarities\n",
    "    df['Best_ID_Jump'] = best_ids\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_match_parallel(data_dict['TP53'], df_jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üî• Application sur toutes les cibles du dictionnaire avec joblib\n",
    "for target in data_dict:\n",
    "    print(f\"üöÄ Processing {target} with multiprocessing...\")\n",
    "    data_dict[target] = find_best_match_parallel(data_dict[target], df_jump)\n",
    "\n",
    "# V√©rification\n",
    "for key, df in data_dict.items():\n",
    "    print(f\"\\nüìÇ {key} - Extrait avec similarit√©s\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les r√©sultats\n",
    "similarity_results = {}\n",
    "list_of_fps_jump = df_jump['Fps'].to_list()\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    query_smiles = row['SMILES'] \n",
    "    similarities = compute_similarity(query_smiles, list_of_fps_jump) \n",
    "    \n",
    "    # Stocker temporairement les similarit√©s sous forme de colonne\n",
    "    similarity_results[f'TC_to_{row[\"ZINC ID\"]}'] = similarities\n",
    "\n",
    "# Conversion du dictionnaire en DataFrame\n",
    "similarity_df = pd.DataFrame(similarity_results)\n",
    "similarity_df.index = df_jump.index  # Assurer l'alignement des index\n",
    "\n",
    "# Fusionner le tout avec df_jump\n",
    "final_df = pd.concat([df_jump, similarity_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_columns = [col for col in final_df.columns if col.startswith(\"TC_to_\")]\n",
    "\n",
    "# Trouver la similarit√© maximale et l'identifiant correspondant\n",
    "df_d4[\"Max_TC\"] = final_df[tc_columns].max(axis=1)  # Valeur TC max\n",
    "df_d4[\"Best_JCPID\"] = final_df[tc_columns].idxmax(axis=1).str.replace(\"TC_to_\", \"\")  # JCPID associ√©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d4[\"Max_TC\"].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
